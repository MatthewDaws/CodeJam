{
 "metadata": {
  "name": "",
  "signature": "sha256:a9eda1b471cd72f974f45fc0ef101cd830279aafe320c0739b76eaae3f84e760"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Random Permutations"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%matplotlib inline\n",
      "import matplotlib.pyplot as plt\n",
      "import numpy as np"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 1
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "This is not so different to the [official contest analysis](https://code.google.com/codejam/contest/2984486/dashboard#s=a&a=2).\n",
      "\n",
      "TODO: Maybe use scikit-learn at the end??"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# The two algorithms\n",
      "import random\n",
      "\n",
      "def pick_bad(N):\n",
      "    a = list(range(N))\n",
      "    for i in range(N):\n",
      "        j = random.randrange(N)\n",
      "        x, y = a[i], a[j]\n",
      "        a[i], a[j] = y, x\n",
      "    return a\n",
      "\n",
      "def pick_good(N):\n",
      "    a = list(range(N))\n",
      "    for i in range(N):\n",
      "        j = random.randrange(i,N)\n",
      "        x, y = a[i], a[j]\n",
      "        a[i], a[j] = y, x\n",
      "    return a    "
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 2
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "What makes the \"bad\" algorithm bad?\n",
      "\n",
      "Well, what makes the \"good\" algorithm good?  In the good algorithm, index `i` is swapped with a (uniformly) random choice between `i` and `N-1`.  This means that 0 is maybe swapped, but then index 0 is fixed; then maybe index 1 is swapped, but then it's fixed.  So, given some permutation a[0...N-1], how can the \"good\" algorithm form it?  The 1st stage must swap 0 and a[0], then a[0] stays fixed.  Change of this is 1/N.  The 2nd step swaps 1 and a[1], and then a[1] is fixed.  Change is 1/(N-1).  And so on, each stage independent, so overall chance is $1/N!$ as hoped.\n",
      "\n",
      "For the \"bad\" algorithm, at each stage we swap `i` with anything.  Naively, comparing this to the good algorithm, small indexes have a higher chance being swapped again, so `a[i]` is less likely to be small relative to `i`."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
      "\n",
      "N = 1000\n",
      "trials = 10000\n",
      "\n",
      "goodcounts = []\n",
      "for _ in range(trials):\n",
      "    a = pick_good(N)\n",
      "    goodcounts.append( sum( a[i]<=i for i in range(N) ) )\n",
      "axes[0].hist(goodcounts)\n",
      "\n",
      "badcounts = []\n",
      "for _ in range(trials):\n",
      "    a = pick_bad(N)\n",
      "    badcounts.append( sum( a[i]<=i for i in range(N) ) )\n",
      "axes[1].hist(badcounts)\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 72,
       "text": [
        "(array([    5.,    70.,   380.,  1341.,  2145.,  3006.,  1991.,   835.,\n",
        "          203.,    24.]),\n",
        " array([ 436. ,  442.8,  449.6,  456.4,  463.2,  470. ,  476.8,  483.6,\n",
        "         490.4,  497.2,  504. ]),\n",
        " <a list of 10 Patch objects>)"
       ]
      },
      {
       "metadata": {},
       "output_type": "display_data",
       "png": "iVBORw0KGgoAAAANSUhEUgAAA0AAAAFwCAYAAACRhhvGAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nRU5ErkJggg==\n",
       "text": [
        "<matplotlib.figure.Figure at 0x6cea748>"
       ]
      }
     ],
     "prompt_number": 72
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "So, for the \"bad\" algorithm, the count of `i` with `a[i]<=i` is centred around 470, while for \"good\" it's close to 500.\n",
      "\n",
      "The buzzword here is \"classifier\" (see [Statistical Classifier)[https://en.wikipedia.org/wiki/Statistical_classification)) so we shall say that if this count is `<= threshold` then guess \"bad\", otherwise guess \"good\".  To find the value of `threshold` we'll run some trials, and pick `threshold` so that the number of incorrect classifications is as small as possible."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def wrong_count(threshold, goodcounts, badcounts):\n",
      "    return sum( t <= threshold for t in goodcounts ) + sum( t > threshold for t in badcounts )\n",
      "\n",
      "candidates = [ (t, wrong_count(t, goodcounts, badcounts)) for t in range(460, 510) ]\n",
      "threshold = min(candidates, key = lambda pair: pair[1])\n",
      "threshold"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 73,
       "text": [
        "(486, 1172)"
       ]
      }
     ],
     "prompt_number": 73
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "So in this case, out of 20000 trials, we'd misclassify 1172 cases, or around 5.9%.  That _should_ be good enough for the problem, with some luck!  The following implements this, and in 20 trials, in only 1 case would we do badly!"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def basic_classifier(a):\n",
      "    stat = sum( a[i]<=i for i in range(N) )\n",
      "    if stat <= 486:\n",
      "        return \"BAD\"\n",
      "    return \"GOOD\"\n",
      "\n",
      "badbad = sum( basic_classifier(pick_bad(N)) == \"GOOD\" for _ in range(10000) )\n",
      "print(\"False rate on BAD is:\",badbad,\"out of 10000\")\n",
      "goodbad = sum( basic_classifier(pick_good(N)) == \"BAD\" for _ in range(10000) )\n",
      "print(\"False rate on GOOD is:\",goodbad,\"out of 10000\")"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "False rate on BAD is: 590 out of 10000\n",
        "False rate on GOOD is:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 596 out of 10000\n"
       ]
      }
     ],
     "prompt_number": 75
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def simulate(N, T, G, classifier):\n",
      "    correct = 0\n",
      "    for _ in range(T):\n",
      "        if random.randrange(2) == 0:\n",
      "            a = pick_good(N)\n",
      "            b = \"GOOD\"\n",
      "        else:\n",
      "            a = pick_bad(N)\n",
      "            b = \"BAD\"\n",
      "        c = classifier(a)\n",
      "        if (c == \"BAD\" and b == \"BAD\") or (c==\"GOOD\" and b==\"GOOD\"):\n",
      "            correct += 1\n",
      "    print(correct, G)\n",
      "    return correct >= G\n",
      "\n",
      "for _ in range(20):\n",
      "    simulate(1000, 120, 109, basic_classifier)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "109 109\n",
        "112"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 109\n",
        "114"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 109\n",
        "114"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 109\n",
        "113"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 109\n",
        "111"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 109\n",
        "116"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 109\n",
        "113"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 109\n",
        "112"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 109\n",
        "112"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 109\n",
        "117"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 109\n",
        "110"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 109\n",
        "108"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 109\n",
        "115"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 109\n",
        "112"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 109\n",
        "114"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 109\n",
        "110"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 109\n",
        "109"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 109\n",
        "115"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 109\n",
        "111"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 109\n"
       ]
      }
     ],
     "prompt_number": 77
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Naive Bayes"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "This is quite a fun idea.\n",
      "\n",
      "Firstly, let's use Bayes's theorem to derive the optimal classifier.  We have data $a = (a_0,\\cdots,a_{N-1})$ and want to know `good` or `bad`.  The prior probability, told to us in the question, is $\\mathbb P(\\text{G}) = \\mathbb P(\\text{B}) = 1/2$.  So\n",
      "$$ \\mathbb P(\\text{B} | a) \\mathbb P(a) = \\mathbb P(a | \\text{B}) \\mathbb P(\\text{B}). $$\n",
      "As ever, we then calculate $\\mathbb P(a) = \\mathbb P(a|\\text{B}) \\mathbb P(\\text{B}) + \\mathbb P(a|\\text{G}) \\mathbb P(\\text{G})$ and so we obtain\n",
      "$$ \\mathbb P(\\text{B} | a)  = \\frac{ \\mathbb P(a | \\text{B}) \\mathbb P(\\text{B}) }{ \\mathbb P(a|\\text{B}) \\mathbb P(\\text{B}) + \\mathbb P(a|\\text{G}) \\mathbb P(\\text{G}) } =  \\frac{ \\mathbb P(a | \\text{B}) }{ \\mathbb P(a|\\text{B}) + \\mathbb P(a|\\text{G}) }$$\n",
      "in this case.\n",
      "\n",
      "So, supposing we know $\\mathbb P(a | \\text{B})$ and $\\mathbb P(a | \\text{G})$, we find $\\mathbb P(\\text{B}|a)$.  Using a \"maximum likelihood\" criteria, we guess \"BAD\" if $\\mathbb P(\\text{B}|a) \\geq 1/2$.  This is equivalent to $\\mathbb P(a | \\text{B}) \\geq \\mathbb P(a | \\text{G})$.  Finally, we know that $\\mathbb P(a | \\text{G}) = 1/N!$.\n",
      "\n",
      "The problem is that working out $\\mathbb P(a | \\text{B})$ seems impossible, and for large N, it's also utterly impossible to even simulate."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "This is where the [naive Bayes](https://en.wikipedia.org/wiki/Naive_Bayes_classifier) idea comes in.  We can use Conditional probability to find\n",
      "$$ \\mathbb P(a|\\text{B}) = \\mathbb P(a_0,\\cdots,a_{N-1}|\\text{B})\n",
      "= \\mathbb P(a_0|\\text{B}) \\mathbb P(a_1,\\cdots,a_{N-1}|a_0,\\text{B})\n",
      "= \\mathbb P(a_0|\\text{B}) \\mathbb P(a_1|a_0,\\text{B}) \\mathbb P(a_2,\\cdots,a_{N-1}|a_0,a_1,\\text{B})\n",
      "= \\cdots $$\n",
      "Now, this doesn't help!  But, let us \"naively\" assume that $a_0, a_1, \\cdots, a_{N-1}$ are independent (of course, they are not, but bear with us).  So we get\n",
      "$$ \\mathbb P(a|\\text{B}) = \\prod_{k=0}^{N-1} \\mathbb P(a_k|\\text{B}). $$\n",
      "So we simply need to work out, or simulate, the $N^2$ probabilities $\\mathbb P(a_j=k|\\text{B})$ for $0\\leq j,k <N$.\n",
      "\n",
      "**Note:** A mistake I made initially was to _not_ also impose the \"naive\" condition on calculating $\\mathbb P(a|\\text{G})$, but we should do this.  However, this is easy to do, as $\\mathbb P(a_j=k|\\text{G}) = 1/N$ for all $j,k$"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def calc_probs(N, k):\n",
      "    \"\"\"Returns prob a[j] == k for varying j\"\"\"\n",
      "    prob = np.zeros(N, dtype=np.float64)\n",
      "    prob[k] = 1.0\n",
      "    for i in range(N):\n",
      "        val = prob[i] / N\n",
      "        prob = prob*(1-1/N) + val\n",
      "        prob[i] = 1/N\n",
      "    return prob\n",
      "\n",
      "# probs[k][j] = prob(a[j] == k | BAD)\n",
      "probs = [calc_probs(N,k) for k in range(N)]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 78
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import math\n",
      "\n",
      "def nb_classifier(a):\n",
      "    global probs\n",
      "    prob = sum( math.log( probs[a[j]][j] ) for j in range(len(a)) )\n",
      "    probgood = -math.log(N) * N\n",
      "    #print(prob, probgood)\n",
      "    if prob >= probgood:\n",
      "        return \"BAD\"\n",
      "    return \"GOOD\"\n",
      "\n",
      "badbad = sum( nb_classifier(pick_bad(N)) == \"GOOD\" for _ in range(10000) )\n",
      "print(\"False rate on BAD is:\",badbad,\"out of 10000\")\n",
      "goodbad = sum( nb_classifier(pick_good(N)) == \"BAD\" for _ in range(10000) )\n",
      "print(\"False rate on GOOD is:\",goodbad,\"out of 10000\")"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "False rate on BAD is: 385 out of 10000\n",
        "False rate on GOOD is:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 375 out of 10000\n"
       ]
      }
     ],
     "prompt_number": 80
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The \"basic classifier\" had a 6% error rate for both good and bad, whereas this has a 4% error rate."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}